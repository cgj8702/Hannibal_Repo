{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Screenplay Proofreading with Gemma 2 9B IT (4-bit)\n",
                "\n",
                "This notebook runs the proofreading process using a free T4 or P100 GPU on Google Colab or Kaggle.\n",
                "\n",
                "### Instructions:\n",
                "1.  **Runtime**: Ensure you are connected to a GPU Runtime (Colab: Runtime > Change runtime type > T4 | Kaggle: Settings > Accelerator > GPU T4 x2).\n",
                "2.  **Files**: Upload your `ParsedScreenplays.zip` file.\n",
                "3.  **Secrets**: Add a secret named `HF_TOKEN` with your Hugging Face token.\n",
                "4.  **Run All**: Run all cells in order."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install necessary libraries\n",
                "!pip install -q -U transformers accelerate bitsandbytes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import login\n",
                "import os\n",
                "\n",
                "try:\n",
                "    # Try Colab Secrets\n",
                "    from google.colab import userdata\n",
                "    hf_token = userdata.get('HF_TOKEN')\n",
                "    login(token=hf_token)\n",
                "    print(\"Logged in via Colab Secret 'HF_TOKEN'!\")\n",
                "except ImportError:\n",
                "    # Try Kaggle Secrets\n",
                "    try:\n",
                "        from kaggle_secrets import UserSecretsClient\n",
                "        user_secrets = UserSecretsClient()\n",
                "        hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
                "        login(token=hf_token)\n",
                "        print(\"Logged in via Kaggle Secret 'HF_TOKEN'!\")\n",
                "    except Exception:\n",
                "        print(\"Secret 'HF_TOKEN' not found. Logging in interactively...\")\n",
                "        login()\n",
                "except Exception:\n",
                "    print(\"Secret 'HF_TOKEN' not found. Logging in interactively...\")\n",
                "    login()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Unzip the uploaded screenplays\n",
                "import zipfile\n",
                "import glob\n",
                "\n",
                "# Handle Kaggle Dataset paths if zip isn't in working dir\n",
                "zip_path = 'ParsedScreenplays.zip'\n",
                "if not os.path.exists(zip_path):\n",
                "    # Check common Kaggle input paths if user uploaded as dataset\n",
                "    possible_paths = glob.glob('/kaggle/input/**/*.zip', recursive=True)\n",
                "    if possible_paths:\n",
                "        zip_path = possible_paths[0]\n",
                "        print(f\"Found zip at {zip_path}\")\n",
                "\n",
                "if os.path.exists(zip_path):\n",
                "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
                "        zip_ref.extractall('.')\n",
                "    print(\"Unzipped ParsedScreenplays!\")\n",
                "else:\n",
                "    print(\"WARNING: ParsedScreenplays.zip not found. Please upload it!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "\n",
                "# SWITCHING TO GEMMA 2 9B to resolve CUDA errors with Gemma 3 4B\n",
                "model_id = \"google/gemma-2-9b-it\"\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16\n",
                ")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
                "\n",
                "print(f\"Model {model_id} loaded successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import glob\n",
                "import re\n",
                "import tqdm\n",
                "\n",
                "def split_into_chunks(text, max_chars=6000):\n",
                "    # Split by scene headers to respect context\n",
                "    scene_pattern = re.compile(r'(=== \\d+ .*? ===)')\n",
                "    parts = scene_pattern.split(text)\n",
                "    chunks = []\n",
                "    current_chunk = \"\"\n",
                "    if parts:\n",
                "         current_chunk = parts[0]\n",
                "         for i in range(1, len(parts), 2):\n",
                "             header = parts[i]\n",
                "             content = parts[i+1] if i+1 < len(parts) else \"\"\n",
                "             full_scene = header + content\n",
                "             if len(current_chunk) + len(full_scene) > max_chars:\n",
                "                 chunks.append(current_chunk)\n",
                "                 current_chunk = full_scene\n",
                "             else:\n",
                "                 current_chunk += full_scene\n",
                "         if current_chunk:\n",
                "             chunks.append(current_chunk)\n",
                "    else:\n",
                "        chunks = [text[i:i+max_chars] for i in range(0, len(text), max_chars)]\n",
                "    return chunks\n",
                "\n",
                "def proofread_chunk(text, filename, chunk_index):\n",
                "    prompt = (\n",
                "        \"You are a professional proofreader. Proofread the following screenplay text segment, \"\n",
                "        \"identifying typos, formatting inconsistencies, or logical errors. \"\n",
                "        \"For each error you find:\\n\"\n",
                "        \"1. Cite the Episode (from filename) and Scene (from headers).\\n\"\n",
                "        \"2. Explain the error.\\n\"\n",
                "        \"3. Provide the corrected text for that section.\\n\\n\"\n",
                "        \"After listing the errors, provide the full validated and corrected version of the text segment. \"\n",
                "        \"Do not leave out any part of the original text in your corrected version.\\n\\n\"\n",
                "        f\"Filename: {filename} (Chunk {chunk_index})\\n\\n\"\n",
                "        \"Screenplay Content:\\n\"\n",
                "        f\"{text}\"\n",
                "    )\n",
                "\n",
                "    messages = [\n",
                "        {\"role\": \"user\", \"content\": prompt}\n",
                "    ]\n",
                "\n",
                "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
                "\n",
                "    outputs = model.generate(\n",
                "        input_ids,\n",
                "        max_new_tokens=4000,\n",
                "        do_sample=True,\n",
                "        temperature=0.7,\n",
                "        top_p=0.95\n",
                "    )\n",
                "\n",
                "    response = outputs[0][input_ids.shape[-1]:]\n",
                "    return tokenizer.decode(response, skip_special_tokens=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Main Processing Loop\n",
                "\n",
                "input_dir = \"ParsedScreenplays\"\n",
                "output_dir = \"ProofreadScreenplays\"\n",
                "\n",
                "if not os.path.exists(output_dir):\n",
                "    os.makedirs(output_dir)\n",
                "\n",
                "files = sorted(glob.glob(os.path.join(input_dir, \"*.txt\")))\n",
                "print(f\"Found {len(files)} screenplays.\")\n",
                "\n",
                "for file_path in tqdm.tqdm(files, desc=\"Proofreading\"):\n",
                "    filename = os.path.basename(file_path)\n",
                "    output_path = os.path.join(output_dir, f\"Proofread_{filename}\")\n",
                "\n",
                "    if os.path.exists(output_path):\n",
                "        print(f\"Skipping {filename} (exists)\")\n",
                "        continue\n",
                "\n",
                "    with open(file_path, 'r', encoding='utf-8') as f:\n",
                "        content = f.read()\n",
                "\n",
                "    chunks = split_into_chunks(content, max_chars=6000)\n",
                "    print(f\"\\nProcessing {filename} in {len(chunks)} chunks...\")\n",
                "\n",
                "    with open(output_path, 'w', encoding='utf-8') as f:\n",
                "        f.write(f\"Proofreading Output for {filename}\\n\")\n",
                "\n",
                "    for i, chunk in enumerate(chunks):\n",
                "        # Retry logic could be added here, but usually local/cloud inference is stable unless OOM\n",
                "        try:\n",
                "            result = proofread_chunk(chunk, filename, i+1)\n",
                "            with open(output_path, 'a', encoding='utf-8') as f:\n",
                "                f.write(f\"\\n=== Chunk {i+1} Output ===\\n{result}\\n\")\n",
                "            print(f\"Done Chunk {i+1}/{len(chunks)}\")\n",
                "        except Exception as e:\n",
                "            print(f\"Error on chunk {i+1}: {e}\")\n",
                "\n",
                "    print(f\"Finished {filename}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Zip results for download\n",
                "!zip -r ProofreadScreenplays.zip ProofreadScreenplays\n",
                "print(\"Zipped results! Downloading... (If on Colab)\")\n",
                "\n",
                "try:\n",
                "    from google.colab import files\n",
                "    files.download('ProofreadScreenplays.zip')\n",
                "except ImportError:\n",
                "    print(\"On Kaggle: Output file 'ProofreadScreenplays.zip' is in the working directory.\")\n",
                "    # Kaggle auto-saves outputs, no explicit download call needed"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}